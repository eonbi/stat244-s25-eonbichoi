[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Built this for my STAT 244 Computational Stats class!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hey! I’m Eonbi. Welcome to my website. I’m currently a senior at Mount Holyoke College studying Statistics. Right now, I’m learning about: Causal inference, Computational statistics, Mathematical statistics, and Sports analytics."
  },
  {
    "objectID": "stat_244_sc.html",
    "href": "stat_244_sc.html",
    "title": "STAT 244-SC",
    "section": "",
    "text": "Our project explores the relationship between pit stops and variables such as lap time, lap number, tire age, and type of tire. These are factors that often play a clear role in when teams choose to make a stop. We are interested in predicting the probability of making a pit stop during the 2024 Miami Grand Prix, considering factors such as lap time, track progress, tire age, and the type of tire used.\nThe data used in this study were obtained from the f1dataR R package that accesses Formula 1 data via the FastF1 Python library. The dataset includes lap-by-lap session data from the 2024 Miami Grand Prix and comprise 1,111 laps and 32 variables. These variables include driver details, lap times, pit in/out times, tire information, and track status. More detailed information about this package can be found in its API documentation.\n\n\nThe frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race.\n\nThe density of lap times for each team during the race. We can compare the performance and variability in lap times across different teams.\n\nThe distribution of tire life (measured in laps) for each tire compound used in the race.\n\nStay tuned for more!"
  },
  {
    "objectID": "stat_244_sc.html#some-visualizations",
    "href": "stat_244_sc.html#some-visualizations",
    "title": "STAT 244-SC",
    "section": "",
    "text": "The frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race.\n\nThe density of lap times for each team during the race. We can compare the performance and variability in lap times across different teams.\n\nThe distribution of tire life (measured in laps) for each tire compound used in the race.\n\nStay tuned for more!"
  },
  {
    "objectID": "Final_Paper.html",
    "href": "Final_Paper.html",
    "title": "Final Paper",
    "section": "",
    "text": "In sports, data is collected to record athletes’ performances, which helps in making decisions to improve outcomes. In Formula 1 and other motorsports, tire management plays an important role in race strategy. Tires directly impact car performance as they are in contact with the track and transfers all the car’s power and movements to the track surface. Tire degradation influences major decisions such as pit stop timing during a race, where minor misjudgments can result in serious consequences in a fast-paced sports like Formula 1. Our study evaluates the relationship between pit stop laps and tire and lap information though employing linear regression models, cross validation, k-fold cross validation, and logistic regression. The objective is to improve pit stop timing prediction to facilitate effective strategic planning for optimizing pit stop timing, ultimately giving drivers a decisive advantage in race management.\n\n\n\nThe data used in this study were obtained from the f1dataR R package that accesses Formula 1 data via the FastF1 Python library. The dataset includes lap-by-lap session data from the 2024 Miami Grand Prix and comprise 1,111 laps and 32 variables. These variables include driver details, lap times, pit in/out times, tire information, and track status. More detailed information about this package can be found in its API documentation.\n\n\n# A tibble: 6 × 32\n   time driver driver_number lap_time lap_number stint pit_out_time pit_in_time\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 3438. VER    1                 94.3          1     1          NaN         NaN\n2 3531. VER    1                 93.1          2     1          NaN         NaN\n3 3624. VER    1                 93.1          3     1          NaN         NaN\n4 3717. VER    1                 93.5          4     1          NaN         NaN\n5 3810. VER    1                 92.8          5     1          NaN         NaN\n6 3903. VER    1                 92.9          6     1          NaN         NaN\n# ℹ 24 more variables: sector1time &lt;dbl&gt;, sector2time &lt;dbl&gt;, sector3time &lt;dbl&gt;,\n#   sector1session_time &lt;dbl&gt;, sector2session_time &lt;dbl&gt;,\n#   sector3session_time &lt;dbl&gt;, speed_i1 &lt;dbl&gt;, speed_i2 &lt;dbl&gt;, speed_fl &lt;dbl&gt;,\n#   speed_st &lt;dbl&gt;, is_personal_best &lt;list&gt;, compound &lt;chr&gt;, tyre_life &lt;dbl&gt;,\n#   fresh_tyre &lt;lgl&gt;, team &lt;chr&gt;, lap_start_time &lt;dbl&gt;, lap_start_date &lt;dttm&gt;,\n#   track_status &lt;chr&gt;, position &lt;dbl&gt;, deleted &lt;lgl&gt;, deleted_reason &lt;chr&gt;,\n#   fast_f1generated &lt;lgl&gt;, is_accurate &lt;lgl&gt;, session_type &lt;chr&gt;\n\n\n\n\n\n\n\n\nlap_time: Recorded time to complete a lap (seconds)\nlap_number: Lap number from which the telemetry data was recorded (number of laps)\ntyre_life: Number of laps completed on a set of tires (number of laps)\n\n\n\n\n\ncompound: Type of tire used (SOFT, MEDIUM, HARD)\npit_in: Whether a driver made a pit stop during a lap (binary: 0 = no pit stop, 1 = pit stop occured)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"There are  5  missing lap time values\"\n\n\nThe dataset contains five missing lap times. Out of 5 missing lap time records four records have a track status code of 41. However, no description of this code value is provided in the API. Thus, we assume that either the track was not fully cleared or conditions were not suitable for racing. The other missing record was due to a driver failing to complete a lap due to collision. Since the missing observations are less than 0.1% of the entire observation, we decided to drop these records.\n\n\n\nDistribution of Pit Stops by Lap\nThis plot shows the frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race. Many teams pitted to change tires during the first half of the race and the most common pit stop occurring around lap 28. This race was unique in that some drivers performed a one-stop strategy, while others went for a two-stop approach. These decisions were influenced by various factors such as track position, gaps to nearby drivers, tire condition, and more. Pit stops in the later stages of the race likely reflect either a two-stop strategy or an attempt to set the fastest lap and earn an extra point.\n\n\n\n\n\n\n\n\n\nDensity of Lap Times by Team\nThis plot shows the distribution of lap times for each team during the race. We can compare the performance and variability in lap times across different teams. For most teams, the lap times are generally under 100 seconds, with some laps approaching 110 seconds. These patterns are largely consistent across teams, though some, such as Mercedes and Williams, show a few outliers on the higher end, which indicates occasional slower laps.\n\n\n\n\n\n\n\n\n\nDistribution of Tire Life by Tire Compound\nThe plot shows the distribution of tire life (measured in laps) for each tire compound used in the race. On average, hard tires lasted very slightly longer than medium tires. Since hard and medium compounds were the most commonly used in this race, we have limited data on soft tires, roughly a quarter as much. This resulted in a narrower distribution for the soft compound. The tire compound directly affects tire life, with a trade-off between performance (speed and grip) and durability. As a result, softer compounds tend to wear out more quickly than harder ones."
  },
  {
    "objectID": "Final_Paper.html#abstract",
    "href": "Final_Paper.html#abstract",
    "title": "Final Paper",
    "section": "",
    "text": "In sports, data is collected to record athletes’ performances, which helps in making decisions to improve outcomes. In Formula 1 and other motorsports, tire management plays an important role in race strategy. Tires directly impact car performance as they are in contact with the track and transfers all the car’s power and movements to the track surface. Tire degradation influences major decisions such as pit stop timing during a race, where minor misjudgments can result in serious consequences in a fast-paced sports like Formula 1. Our study evaluates the relationship between pit stop laps and tire and lap information though employing linear regression models, cross validation, k-fold cross validation, and logistic regression. The objective is to improve pit stop timing prediction to facilitate effective strategic planning for optimizing pit stop timing, ultimately giving drivers a decisive advantage in race management."
  },
  {
    "objectID": "Final_Paper.html#dataset",
    "href": "Final_Paper.html#dataset",
    "title": "Final Paper",
    "section": "",
    "text": "The data used in this study were obtained from the f1dataR R package that accesses Formula 1 data via the FastF1 Python library. The dataset includes lap-by-lap session data from the 2024 Miami Grand Prix and comprise 1,111 laps and 32 variables. These variables include driver details, lap times, pit in/out times, tire information, and track status. More detailed information about this package can be found in its API documentation.\n\n\n# A tibble: 6 × 32\n   time driver driver_number lap_time lap_number stint pit_out_time pit_in_time\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 3438. VER    1                 94.3          1     1          NaN         NaN\n2 3531. VER    1                 93.1          2     1          NaN         NaN\n3 3624. VER    1                 93.1          3     1          NaN         NaN\n4 3717. VER    1                 93.5          4     1          NaN         NaN\n5 3810. VER    1                 92.8          5     1          NaN         NaN\n6 3903. VER    1                 92.9          6     1          NaN         NaN\n# ℹ 24 more variables: sector1time &lt;dbl&gt;, sector2time &lt;dbl&gt;, sector3time &lt;dbl&gt;,\n#   sector1session_time &lt;dbl&gt;, sector2session_time &lt;dbl&gt;,\n#   sector3session_time &lt;dbl&gt;, speed_i1 &lt;dbl&gt;, speed_i2 &lt;dbl&gt;, speed_fl &lt;dbl&gt;,\n#   speed_st &lt;dbl&gt;, is_personal_best &lt;list&gt;, compound &lt;chr&gt;, tyre_life &lt;dbl&gt;,\n#   fresh_tyre &lt;lgl&gt;, team &lt;chr&gt;, lap_start_time &lt;dbl&gt;, lap_start_date &lt;dttm&gt;,\n#   track_status &lt;chr&gt;, position &lt;dbl&gt;, deleted &lt;lgl&gt;, deleted_reason &lt;chr&gt;,\n#   fast_f1generated &lt;lgl&gt;, is_accurate &lt;lgl&gt;, session_type &lt;chr&gt;"
  },
  {
    "objectID": "Final_Paper.html#variables-of-interest",
    "href": "Final_Paper.html#variables-of-interest",
    "title": "Final Paper",
    "section": "",
    "text": "lap_time: Recorded time to complete a lap (seconds)\nlap_number: Lap number from which the telemetry data was recorded (number of laps)\ntyre_life: Number of laps completed on a set of tires (number of laps)\n\n\n\n\n\ncompound: Type of tire used (SOFT, MEDIUM, HARD)\npit_in: Whether a driver made a pit stop during a lap (binary: 0 = no pit stop, 1 = pit stop occured)"
  },
  {
    "objectID": "Final_Paper.html#missing-data-in-lap_time",
    "href": "Final_Paper.html#missing-data-in-lap_time",
    "title": "Final Paper",
    "section": "",
    "text": "[1] \"There are  5  missing lap time values\"\n\n\nThe dataset contains five missing lap times. Out of 5 missing lap time records four records have a track status code of 41. However, no description of this code value is provided in the API. Thus, we assume that either the track was not fully cleared or conditions were not suitable for racing. The other missing record was due to a driver failing to complete a lap due to collision. Since the missing observations are less than 0.1% of the entire observation, we decided to drop these records."
  },
  {
    "objectID": "Final_Paper.html#exploratory-data-analysis-visualization",
    "href": "Final_Paper.html#exploratory-data-analysis-visualization",
    "title": "Final Paper",
    "section": "",
    "text": "Distribution of Pit Stops by Lap\nThis plot shows the frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race. Many teams pitted to change tires during the first half of the race and the most common pit stop occurring around lap 28. This race was unique in that some drivers performed a one-stop strategy, while others went for a two-stop approach. These decisions were influenced by various factors such as track position, gaps to nearby drivers, tire condition, and more. Pit stops in the later stages of the race likely reflect either a two-stop strategy or an attempt to set the fastest lap and earn an extra point.\n\n\n\n\n\n\n\n\n\nDensity of Lap Times by Team\nThis plot shows the distribution of lap times for each team during the race. We can compare the performance and variability in lap times across different teams. For most teams, the lap times are generally under 100 seconds, with some laps approaching 110 seconds. These patterns are largely consistent across teams, though some, such as Mercedes and Williams, show a few outliers on the higher end, which indicates occasional slower laps.\n\n\n\n\n\n\n\n\n\nDistribution of Tire Life by Tire Compound\nThe plot shows the distribution of tire life (measured in laps) for each tire compound used in the race. On average, hard tires lasted very slightly longer than medium tires. Since hard and medium compounds were the most commonly used in this race, we have limited data on soft tires, roughly a quarter as much. This resulted in a narrower distribution for the soft compound. The tire compound directly affects tire life, with a trade-off between performance (speed and grip) and durability. As a result, softer compounds tend to wear out more quickly than harder ones."
  },
  {
    "objectID": "Final_Paper.html#research-questions",
    "href": "Final_Paper.html#research-questions",
    "title": "Final Paper",
    "section": "Research questions",
    "text": "Research questions\n\nWere drivers more likely to make pit stops when their lap time was longer and their tires were older compared to when their lap time was shorter and their tires were less used?\nWere drivers more likely to make pit stops when their lap times were longer, their tires were older, and considering the type of tires they were using and their progress in the race?"
  },
  {
    "objectID": "Final_Paper.html#linear-models",
    "href": "Final_Paper.html#linear-models",
    "title": "Final Paper",
    "section": "Linear Models",
    "text": "Linear Models\n\nModel 1:\n\n\\[\\mathbb{E}(pit\\_in \\mid lap\\_time,\\ tyre\\_life) = \\beta_0 + \\beta_1(lap\\_time) + \\beta_2(tyre\\_life)\\]\n\nModel 2:\n\n\\[\n\\begin{aligned}\n\\mathbb{E}(pit\\_in \\mid lap\\_time, \\ lap\\_number, \\ compound, \\ tyre\\_life)  &= \\beta_0 + \\beta_1(lap\\_time) \\\\ &+ \\beta_2(lap\\_number) + \\beta_3(compound) \\\\ &+ \\beta_4(tyre\\_life)\n\\end{aligned}\n\\]\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -0.422    0.0543       -7.77 1.74e-14\n2 lap_time     0.00429  0.000537      7.98 3.63e-15\n3 tyre_life    0.00243  0.000509      4.79 1.94e- 6\n\n\n# A tibble: 6 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -0.446    0.0544       -8.21 6.32e-16\n2 lap_time        0.00468  0.000534      8.76 7.32e-18\n3 lap_number     -0.00214  0.000387     -5.54 3.88e- 8\n4 compoundMEDIUM  0.0117   0.00936       1.25 2.10e- 1\n5 compoundSOFT    0.0312   0.0240        1.30 1.93e- 1\n6 tyre_life       0.00519  0.000698      7.44 1.97e-13\n\n\nThe regression results show that drivers were slightly more likely to make pit stops when their lap times were longer and their tires were older. In the extended model, lap time and tire age remained strong predictors and suggested that there are fewer stops later in the race with lap number having a slight negative effect. Tire compound had a small and non-significant effect. This indicates that tire compound did not meaningfully influence pit stop decisions when other factors were considered."
  },
  {
    "objectID": "Final_Paper.html#cross-validation",
    "href": "Final_Paper.html#cross-validation",
    "title": "Final Paper",
    "section": "Cross Validation",
    "text": "Cross Validation\nCross-validation is a statistical method used to evaluate how well a model performs by splitting the data into multiple subsets to train the model on some subsets and validate it on the remaining subsets.\nGoal: Provide a more reliable and unbiased estimate of a model’s performance predicting new data, in order to detect overfitting and improve model generalization\n\nDividing data into test set and training set\nk-fold CV: We can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\nFit a model using the data in the other \\(k-1\\) folds (training).\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\nCalculate the MAE/MSE for fold \\(j\\) (testing):\n\nCombine this information into one measure of model quality\n\n\n\nError metric to use\nMean absolute error (MAE) of an estimator measures the absolute difference between the predicted values and the actual values in the dataset. Its advantage is that its\n\n\\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\)\n\\(\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\)\n\nMean squared error (MSE) of an estimator measures the average squared difference between the predicted values and the actual values in the dataset.\n\n\\(\\text{MSE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} (y_i - \\hat{y}_i)^2\\)\n\\(\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MSE}_j\\)\n\n\nMAE vs. MSE\nThe advantage of using MAE is that it’s more robust to outliers, giving equal weight to all errors. Thus, it’s more suitable when outliers are not a significant concern.\nOn the other hand, MSE gives more weight to larger errors than smaller ones, making it highly sensitive to outliers. MSE is more suitable when the risk of prediction mistakes is crucial and the goal is to minimize the risk of errors.\nSince outliers are less of a concern for us as they don’t lead to any life threatening or other major issues, we prioritize models that are directly interpretable. Our data is less common and less familiar to many people, so we decided to choose a model based on MAE.\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard      0.0505\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard      0.0587"
  },
  {
    "objectID": "Final_Paper.html#k-fold-cv-implementation-for-different-values-of-k",
    "href": "Final_Paper.html#k-fold-cv-implementation-for-different-values-of-k",
    "title": "Final Paper",
    "section": "k-fold CV implementation for different values of k",
    "text": "k-fold CV implementation for different values of k\n\nCase 1: k=5\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0507     5 0.00159 Preprocessor1_Model1\n2 rmse    standard   0.152      5 0.00536 Preprocessor1_Model1\n\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [884/222]&gt; Fold1 mae     standard      0.0477 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [885/221]&gt; Fold2 mae     standard      0.0560 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [885/221]&gt; Fold3 mae     standard      0.0528 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [885/221]&gt; Fold4 mae     standard      0.0486 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [885/221]&gt; Fold5 mae     standard      0.0485 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.048) and worst for fold 2 (0.056).\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0592     5 0.00161 Preprocessor1_Model1\n2 rmse    standard   0.150      5 0.00490 Preprocessor1_Model1\n\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [884/222]&gt; Fold1 mae     standard      0.0533 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [885/221]&gt; Fold2 mae     standard      0.0621 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [885/221]&gt; Fold3 mae     standard      0.0617 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [885/221]&gt; Fold4 mae     standard      0.0585 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [885/221]&gt; Fold5 mae     standard      0.0606 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.053) and worst for fold 2 (0.062).\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0507 0.00356\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0592 0.00360\n\n\nIn-sample and 5-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n5-fold CV MAE\nIn-sample SD\n5-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05073\n0.15247\n0.00356\n\n\nmodel_2\n0.05975\n0.05922\n0.15035\n0.00360\n\n\n\n5-fold cross-validation was used to assess the performance of two models predicting pit stops. Model 1, using only lap time and tire life, achieved a mean MAE of 0.05073 with a low standard deviation (0.00356). Model 2, which adds lap number and tire compound, had a higher mean MAE of 0.05922 with a similar standard deviation (0.00360).\nAlthough Model 2 includes more predictors, it performed slightly worse than Model 1 in both cross-validation and in-sample metrics. This suggests that the additional variables do not improve prediction. Model 1 is therefore more accurate and efficient for predicting pit stops.\n\n\nCase 2: k=10\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0510    10 0.00294 Preprocessor1_Model1\n2 rmse    standard   0.150     10 0.0109  Preprocessor1_Model1\n\n\n# A tibble: 10 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [995/111]&gt; Fold01 mae     standard      0.0368 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [995/111]&gt; Fold02 mae     standard      0.0544 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [995/111]&gt; Fold03 mae     standard      0.0614 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [995/111]&gt; Fold04 mae     standard      0.0472 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [995/111]&gt; Fold05 mae     standard      0.0379 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [995/111]&gt; Fold06 mae     standard      0.0602 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [996/110]&gt; Fold07 mae     standard      0.0600 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [996/110]&gt; Fold08 mae     standard      0.0434 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [996/110]&gt; Fold09 mae     standard      0.0505 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [996/110]&gt; Fold10 mae     standard      0.0581 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, the MAE was best for fold 1 with an MAE of approximately 0.037 and worst for fold 3 with an MAE of 0.061 approximately.\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0594    10 0.00262 Preprocessor1_Model1\n2 rmse    standard   0.148     10 0.0104  Preprocessor1_Model1\n\n\n# A tibble: 10 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [995/111]&gt; Fold01 mae     standard      0.0436 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [995/111]&gt; Fold02 mae     standard      0.0616 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [995/111]&gt; Fold03 mae     standard      0.0698 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [995/111]&gt; Fold04 mae     standard      0.0566 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [995/111]&gt; Fold05 mae     standard      0.0518 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [995/111]&gt; Fold06 mae     standard      0.0658 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [996/110]&gt; Fold07 mae     standard      0.0655 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [996/110]&gt; Fold08 mae     standard      0.0521 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [996/110]&gt; Fold09 mae     standard      0.0601 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [996/110]&gt; Fold10 mae     standard      0.0671 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.044) and worst for fold 3 (0.070).\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0510 0.00931\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0594 0.00829\n\n\nIn-sample and 10-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n10-fold CV MAE\nIn-sample SD\n10-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05100\n0.15247\n0.00931\n\n\nmodel_2\n0.05975\n0.05939\n0.15035\n0.00829\n\n\n\nWith 10-fold cross-validation, Model 1 had a mean MAE of 0.0510, while Model 2 had a slightly higher MAE of 0.0594. Both models showed low standard deviations approximately 0.009. As in the 5-fold case, Model 1 remained slightly more accurate and stable than Model 2.\n\n\nCase 3: k = 20\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0509    20 0.00399 Preprocessor1_Model1\n2 rmse    standard   0.140     20 0.0142  Preprocessor1_Model1\n\n\n# A tibble: 20 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [1050/56]&gt; Fold01 mae     standard      0.0451 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [1050/56]&gt; Fold02 mae     standard      0.0519 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [1050/56]&gt; Fold03 mae     standard      0.0509 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [1050/56]&gt; Fold04 mae     standard      0.0658 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [1050/56]&gt; Fold05 mae     standard      0.0439 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [1050/56]&gt; Fold06 mae     standard      0.0412 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [1051/55]&gt; Fold07 mae     standard      0.0602 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [1051/55]&gt; Fold08 mae     standard      0.0429 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [1051/55]&gt; Fold09 mae     standard      0.0402 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [1051/55]&gt; Fold10 mae     standard      0.0263 Preprocessor1… &lt;tibble&gt;\n11 &lt;split [1051/55]&gt; Fold11 mae     standard      0.0266 Preprocessor1… &lt;tibble&gt;\n12 &lt;split [1051/55]&gt; Fold12 mae     standard      0.0585 Preprocessor1… &lt;tibble&gt;\n13 &lt;split [1051/55]&gt; Fold13 mae     standard      0.0704 Preprocessor1… &lt;tibble&gt;\n14 &lt;split [1051/55]&gt; Fold14 mae     standard      0.0274 Preprocessor1… &lt;tibble&gt;\n15 &lt;split [1051/55]&gt; Fold15 mae     standard      0.0302 Preprocessor1… &lt;tibble&gt;\n16 &lt;split [1051/55]&gt; Fold16 mae     standard      0.0825 Preprocessor1… &lt;tibble&gt;\n17 &lt;split [1051/55]&gt; Fold17 mae     standard      0.0591 Preprocessor1… &lt;tibble&gt;\n18 &lt;split [1051/55]&gt; Fold18 mae     standard      0.0429 Preprocessor1… &lt;tibble&gt;\n19 &lt;split [1051/55]&gt; Fold19 mae     standard      0.0611 Preprocessor1… &lt;tibble&gt;\n20 &lt;split [1051/55]&gt; Fold20 mae     standard      0.0901 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 10 (0.026) and worst for fold 20 (0.090).\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0593    20 0.00398 Preprocessor1_Model1\n2 rmse    standard   0.139     20 0.0134  Preprocessor1_Model1\n\n\n# A tibble: 20 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [1050/56]&gt; Fold01 mae     standard      0.0508 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [1050/56]&gt; Fold02 mae     standard      0.0623 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [1050/56]&gt; Fold03 mae     standard      0.0564 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [1050/56]&gt; Fold04 mae     standard      0.0755 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [1050/56]&gt; Fold05 mae     standard      0.0596 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [1050/56]&gt; Fold06 mae     standard      0.0535 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [1051/55]&gt; Fold07 mae     standard      0.0652 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [1051/55]&gt; Fold08 mae     standard      0.0492 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [1051/55]&gt; Fold09 mae     standard      0.0474 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [1051/55]&gt; Fold10 mae     standard      0.0324 Preprocessor1… &lt;tibble&gt;\n11 &lt;split [1051/55]&gt; Fold11 mae     standard      0.0347 Preprocessor1… &lt;tibble&gt;\n12 &lt;split [1051/55]&gt; Fold12 mae     standard      0.0630 Preprocessor1… &lt;tibble&gt;\n13 &lt;split [1051/55]&gt; Fold13 mae     standard      0.0818 Preprocessor1… &lt;tibble&gt;\n14 &lt;split [1051/55]&gt; Fold14 mae     standard      0.0362 Preprocessor1… &lt;tibble&gt;\n15 &lt;split [1051/55]&gt; Fold15 mae     standard      0.0405 Preprocessor1… &lt;tibble&gt;\n16 &lt;split [1051/55]&gt; Fold16 mae     standard      0.0838 Preprocessor1… &lt;tibble&gt;\n17 &lt;split [1051/55]&gt; Fold17 mae     standard      0.0652 Preprocessor1… &lt;tibble&gt;\n18 &lt;split [1051/55]&gt; Fold18 mae     standard      0.0537 Preprocessor1… &lt;tibble&gt;\n19 &lt;split [1051/55]&gt; Fold19 mae     standard      0.0724 Preprocessor1… &lt;tibble&gt;\n20 &lt;split [1051/55]&gt; Fold20 mae     standard      0.102  Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 10 (0.032) and worst for fold 20 (0.101).\n\n\n# A tibble: 1 × 2\n    mean     sd\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0509 0.0178\n\n\n# A tibble: 1 × 2\n    mean     sd\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0593 0.0178\n\n\nIn-sample and 20-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n20-fold CV MAE\nIn-sample SD\n20-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05086\n0.15247\n0.01785\n\n\nmodel_2\n0.05975\n0.05925\n0.15035\n0.01781\n\n\n\nIn the 20-fold CV setup, Model 1 performed better than Model 2 with a lower mean MAE, 0.05086 and 0.05925, respectively. Even with increased fold, the simpler model generalized better across the dataset.\n\n\nComparison between different values of k\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n5-fold CV MAE\n10-fold CV MAE\n20-fold CV MAE\n\n\n\n\nmodel_1\n0.05045\n0.05073\n0.05100\n0.05086\n\n\nmodel_2\n0.05975\n0.05922\n0.05939\n0.05925\n\n\n\nAcross all cross-validation settings (5, 10, and 20-fold), Model 1 consistently showed lower MAE than Model 2. The differences were small but consistent and this suggests that Model 1 is a better model than Model 2 in predicting pit-stops.\nTherefore, our final model based on the smallest CV error is:\n\\[\\mathbb{E}(pit\\_in \\mid lap\\_time,\\ tyre\\_life) = \\beta_0 + \\beta_1(lap\\_time) + \\beta_2(tyre\\_life)\\]"
  },
  {
    "objectID": "Final_Paper.html#variables-of-interest-1",
    "href": "Final_Paper.html#variables-of-interest-1",
    "title": "Final Paper",
    "section": "Variables of Interest",
    "text": "Variables of Interest\n\nPredictors:\n\nlap_time: Recorded time to complete a lap (seconds).\nlap_number: Lap number from which the telemetry data was recorded (number of laps).\ntyre_life: Number of laps completed on a set of tires (number of laps).\ncompound: Type of tire used (SOFT, MEDIUM, HARD).\n\n\n\nResponse Variable:\n\npit_in: Whether a driver made a pit stop during a lap where 1 indicates pit stop occurred, and 0 otherwise\n\\[\\begin{align*}\nY_i &= \\begin{cases} 1 & \\text{ if a driver pitted on a lap } \\\\ 0 & \\text{ otherwise (i.e., the driver did not pit on lap)} \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "Final_Paper.html#our-logistic-regression-model",
    "href": "Final_Paper.html#our-logistic-regression-model",
    "title": "Final Paper",
    "section": "Our Logistic Regression Model",
    "text": "Our Logistic Regression Model\nWe are interested in determining the probability of making a pit stop during the 2024 Miami Grand Prix, considering factors such as lap time, track progress, tire age, and the type of tire used.\n\\[\n\\begin{aligned}\n\\log(odds(pit\\_in \\mid lap\\_time, \\ lap\\_number, \\ tyre\\_life, \\ compound)) &= \\beta_0 + \\beta_1 (lap\\_time) \\\\ &+ \\beta_2(lap\\_number) + \\beta_3 (tyre\\_life) \\\\ &+ \\beta_4 \\ I(compound = MEDIUM) \\\\ &+ \\beta_5 \\ I(compound = SOFT)\n\\end{aligned}\n\\]\n\n\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -18.48162    2.27137  -8.137 4.06e-16 ***\nlap_time         0.13739    0.02007   6.846 7.58e-12 ***\nlap_number      -0.16001    0.03630  -4.408 1.04e-05 ***\ntyre_life        0.27508    0.04580   6.006 1.91e-09 ***\ncompoundMEDIUM   0.49495    0.49718   0.996    0.319    \ncompoundSOFT     1.86135    1.17923   1.578    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 261.16  on 1105  degrees of freedom\nResidual deviance: 176.46  on 1100  degrees of freedom\nAIC: 188.46\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nInterpretation of exponentiated \\(\\hat{\\beta}\\) coefficients\n\n\n   (Intercept)       lap_time     lap_number      tyre_life compoundMEDIUM \n  9.408757e-09   1.147280e+00   8.521343e-01   1.316630e+00   1.640419e+00 \n  compoundSOFT \n  6.432386e+00 \n\n\n\n\\(\\exp(\\beta_0)\\): The odds of a driver making a pit stop during a lap, when lap time is 0 seconds, lap number is 0, 0 laps have been completed on the current set of tires, and the HARD compound is, is approximately \\(9.4088 \\times 10^{-9}\\).\n\\(\\exp(\\beta_1)\\): For every of 1 second increase in lap time, the odds of a driver pitting increase by a factor of 1.1473.\n\\(\\exp(\\beta_2)\\): For every additional lap (i.e., increase of 1 in the lap number), we expect the odds of a driver pitting to increase by a factor of 0.8521.\n\\(\\exp(\\beta_3)\\): For each additional lap completed on the current set of tires, the odds of a driver pitting increase by a factor of 1.3166.\n\\(\\exp(\\beta_4)\\): When using MEDIUM compound tires instead of HARD, the odds of a driver pitting increase by a factor of 1.6404, holding all other variables constant.\n\\(\\exp(\\beta_5)\\): When using SOFT compound tires instead of HARD, we expect the odds of a driver pitting to increase by a factor of 6.4324, holding all other variables constant.\n\n\nMathematically derive \\(\\exp(\\beta_1)\\)\n\\[\n\\begin{aligned}\n&\\log(odds(pit\\_in \\mid lap\\_time = a)) = -18.4816 + 0.1374a\n\\\\\n\\\\\n&\\log(odds(pit\\_in \\mid lap\\_time = a+1)) = -18.4816 + 0.1374(a+1)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n& \\log\\left( \\frac{odds(pit\\_in \\mid lap\\_time = a+1)}{odds(pit\\_in \\mid lap\\_time = a)} \\right)\\\\\n&= \\log(odds(pit\\_in \\mid lap\\_time = a+1)) - \\log(odds(pit\\_in \\mid lap\\_time = a)) \\\\\n&= (-18.4816 + 0.1374(a+1)) - (-18.4816 + 0.1374) \\\\\n&= 0.1374  \\\\\n&= \\hat{\\beta_1}\n\\end{aligned}\n\\]\nTherefore, \\(\\exp(\\beta_1) = e^{0.1374} = 1.1473\\)\n\n\n\nPredicting High Probability of a Pit Stop\nTo predict a probability of a driver making a pit stop that is very close to 1, we need to input extreme values for the predictors. Based on the five-number summary of our data, we use the following scenario: a lap time of 148.74 seconds, lap number 57, SOFT compound, and a tire age of 45 laps.\n\n\n    lap_time        lap_number      compound     tyre_life    \n Min.   : 90.63   Min.   : 1.00   HARD  :500   Min.   : 1.00  \n 1st Qu.: 92.38   1st Qu.:14.00   MEDIUM:562   1st Qu.: 7.00  \n Median : 93.28   Median :28.00   SOFT  : 44   Median :13.50  \n Mean   : 96.00   Mean   :28.62                Mean   :14.78  \n 3rd Qu.: 94.29   3rd Qu.:43.00                3rd Qu.:22.00  \n Max.   :148.74   Max.   :57.00                Max.   :45.00  \n     pit_in        pit_in_fac\n Min.   :0.00000   0:1078    \n 1st Qu.:0.00000   1:  28    \n Median :0.00000             \n Mean   :0.02532             \n 3rd Qu.:0.00000             \n Max.   :1.00000             \n\n\n\n\n        1 \n0.7308921 \n\n\nUsing our logistic regression model, we estimate the probability of a pit stop under these conditions to be approximately 0.731. This indicates a high likelihood of a pit stop given these extreme race conditions.\n\n\nPredicting Pit Stops with our Logistic Regression Model\n\nEstimate the probability of a driver making a pit stop on a lap with the following conditions: 96.00 seconds lap time, 28th lap, 14.78 laps completed on a set of HARD tires.\n\n\n        1 \n0.5008283 \n\n\n\nThere is approximately a 50.08% probability that a driver will make a pit stop on this lap when using HARD tires, holding all other variables constant.\n\n\nEstimate the probability of a driver making a pit stop on a lap under the same conditions as above but using a set of MEDIUM tires.\n\n\n        1 \n0.5013559 \n\n\n\nWith MEDIUM tires, the probability of making a pit stop increases to 50.14%.\n\n\nEstimate the probability of a driver making a pit stop on a lap under the same conditions as above but using a set of SOFT tires.\n\n\n        1 \n0.5052337 \n\n\n\nWith SOFT tires, the probability increases slightly to 50.52%.\nWhile all the other variables stay the same, we predict that the probability a driver to made a pit stop is higher if the driver is on a set of SOFT tires compared to other compounds."
  },
  {
    "objectID": "Final_Paper.html#proscons-of-logistic-regression-vs.-regular-linear-regression",
    "href": "Final_Paper.html#proscons-of-logistic-regression-vs.-regular-linear-regression",
    "title": "Final Paper",
    "section": "Pros/Cons of logistic regression vs. regular linear regression",
    "text": "Pros/Cons of logistic regression vs. regular linear regression\n\nLogistic Regression\n\n\n\n\n\n\n\nPros\nSince logistic regression is based on a Bernoulli/binomial likelihood, it is a natural model for binary outcomes.\nCoefficients are interpretable in terms of odds ratios (with log-odds as the linear predictor).\n\n\nCons\nThe relationship between predictors and the probability is not linear.\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nPros\nStraightforward linear regression\nEasy to interpret the coefficients\n\n\nCons\nCannot gaurantee that the predicted probabilities to be between 0 and 1."
  },
  {
    "objectID": "final_paper.html",
    "href": "final_paper.html",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "In sports, data is collected to record athletes’ performances, which helps in making decisions to improve outcomes. In Formula 1 and other motorsports, tire management plays an important role in race strategy. Tires directly impact car performance as they are in contact with the track and transfers all the car’s power and movements to the track surface. Tire degradation influences major decisions such as pit stop timing during a race, where minor misjudgments can result in serious consequences in a fast-paced sports like Formula 1. Our study evaluates the relationship between pit stop laps and tire and lap information though employing linear regression models, cross validation, k-fold cross validation, and logistic regression. The objective is to improve pit stop timing prediction to facilitate effective strategic planning for optimizing pit stop timing, ultimately giving drivers a decisive advantage in race management.\n\n\n\nThe data used in this study were obtained from the f1dataR R package that accesses Formula 1 data via the FastF1 Python library. The dataset includes lap-by-lap session data from the 2024 Miami Grand Prix and comprise 1,111 laps and 32 variables. These variables include driver details, lap times, pit in/out times, tire information, and track status. More detailed information about this package can be found in its API documentation.\n\n\n# A tibble: 6 × 32\n   time driver driver_number lap_time lap_number stint pit_out_time pit_in_time\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 3438. VER    1                 94.3          1     1          NaN         NaN\n2 3531. VER    1                 93.1          2     1          NaN         NaN\n3 3624. VER    1                 93.1          3     1          NaN         NaN\n4 3717. VER    1                 93.5          4     1          NaN         NaN\n5 3810. VER    1                 92.8          5     1          NaN         NaN\n6 3903. VER    1                 92.9          6     1          NaN         NaN\n# ℹ 24 more variables: sector1time &lt;dbl&gt;, sector2time &lt;dbl&gt;, sector3time &lt;dbl&gt;,\n#   sector1session_time &lt;dbl&gt;, sector2session_time &lt;dbl&gt;,\n#   sector3session_time &lt;dbl&gt;, speed_i1 &lt;dbl&gt;, speed_i2 &lt;dbl&gt;, speed_fl &lt;dbl&gt;,\n#   speed_st &lt;dbl&gt;, is_personal_best &lt;list&gt;, compound &lt;chr&gt;, tyre_life &lt;dbl&gt;,\n#   fresh_tyre &lt;lgl&gt;, team &lt;chr&gt;, lap_start_time &lt;dbl&gt;, lap_start_date &lt;dttm&gt;,\n#   track_status &lt;chr&gt;, position &lt;dbl&gt;, deleted &lt;lgl&gt;, deleted_reason &lt;chr&gt;,\n#   fast_f1generated &lt;lgl&gt;, is_accurate &lt;lgl&gt;, session_type &lt;chr&gt;\n\n\n\n\n\n\n\n\nlap_time: Recorded time to complete a lap (seconds)\nlap_number: Lap number from which the telemetry data was recorded (number of laps)\ntyre_life: Number of laps completed on a set of tires (number of laps)\n\n\n\n\n\ncompound: Type of tire used (SOFT, MEDIUM, HARD)\npit_in: Whether a driver made a pit stop during a lap (binary: 0 = no pit stop, 1 = pit stop occured)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"There are  5  missing lap time values\"\n\n\nThe dataset contains five missing lap times. Out of 5 missing lap time records four records have a track status code of 41. However, no description of this code value is provided in the API. Thus, we assume that either the track was not fully cleared or conditions were not suitable for racing. The other missing record was due to a driver failing to complete a lap due to collision. Since the missing observations are less than 0.1% of the entire observation, we decided to drop these records.\n\n\n\nDistribution of Pit Stops by Lap\nThis plot shows the frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race. Many teams pitted to change tires during the first half of the race and the most common pit stop occurring around lap 28. This race was unique in that some drivers performed a one-stop strategy, while others went for a two-stop approach. These decisions were influenced by various factors such as track position, gaps to nearby drivers, tire condition, and more. Pit stops in the later stages of the race likely reflect either a two-stop strategy or an attempt to set the fastest lap and earn an extra point.\n\n\n\n\n\n\n\n\n\nDensity of Lap Times by Team\nThis plot shows the distribution of lap times for each team during the race. We can compare the performance and variability in lap times across different teams. For most teams, the lap times are generally under 100 seconds, with some laps approaching 110 seconds. These patterns are largely consistent across teams, though some, such as Mercedes and Williams, show a few outliers on the higher end, which indicates occasional slower laps.\n\n\n\n\n\n\n\n\n\nDistribution of Tire Life by Tire Compound\nThe plot shows the distribution of tire life (measured in laps) for each tire compound used in the race. On average, hard tires lasted very slightly longer than medium tires. Since hard and medium compounds were the most commonly used in this race, we have limited data on soft tires, roughly a quarter as much. This resulted in a narrower distribution for the soft compound. The tire compound directly affects tire life, with a trade-off between performance (speed and grip) and durability. As a result, softer compounds tend to wear out more quickly than harder ones."
  },
  {
    "objectID": "final_paper.html#abstract",
    "href": "final_paper.html#abstract",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "In sports, data is collected to record athletes’ performances, which helps in making decisions to improve outcomes. In Formula 1 and other motorsports, tire management plays an important role in race strategy. Tires directly impact car performance as they are in contact with the track and transfers all the car’s power and movements to the track surface. Tire degradation influences major decisions such as pit stop timing during a race, where minor misjudgments can result in serious consequences in a fast-paced sports like Formula 1. Our study evaluates the relationship between pit stop laps and tire and lap information though employing linear regression models, cross validation, k-fold cross validation, and logistic regression. The objective is to improve pit stop timing prediction to facilitate effective strategic planning for optimizing pit stop timing, ultimately giving drivers a decisive advantage in race management."
  },
  {
    "objectID": "final_paper.html#dataset",
    "href": "final_paper.html#dataset",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "The data used in this study were obtained from the f1dataR R package that accesses Formula 1 data via the FastF1 Python library. The dataset includes lap-by-lap session data from the 2024 Miami Grand Prix and comprise 1,111 laps and 32 variables. These variables include driver details, lap times, pit in/out times, tire information, and track status. More detailed information about this package can be found in its API documentation.\n\n\n# A tibble: 6 × 32\n   time driver driver_number lap_time lap_number stint pit_out_time pit_in_time\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 3438. VER    1                 94.3          1     1          NaN         NaN\n2 3531. VER    1                 93.1          2     1          NaN         NaN\n3 3624. VER    1                 93.1          3     1          NaN         NaN\n4 3717. VER    1                 93.5          4     1          NaN         NaN\n5 3810. VER    1                 92.8          5     1          NaN         NaN\n6 3903. VER    1                 92.9          6     1          NaN         NaN\n# ℹ 24 more variables: sector1time &lt;dbl&gt;, sector2time &lt;dbl&gt;, sector3time &lt;dbl&gt;,\n#   sector1session_time &lt;dbl&gt;, sector2session_time &lt;dbl&gt;,\n#   sector3session_time &lt;dbl&gt;, speed_i1 &lt;dbl&gt;, speed_i2 &lt;dbl&gt;, speed_fl &lt;dbl&gt;,\n#   speed_st &lt;dbl&gt;, is_personal_best &lt;list&gt;, compound &lt;chr&gt;, tyre_life &lt;dbl&gt;,\n#   fresh_tyre &lt;lgl&gt;, team &lt;chr&gt;, lap_start_time &lt;dbl&gt;, lap_start_date &lt;dttm&gt;,\n#   track_status &lt;chr&gt;, position &lt;dbl&gt;, deleted &lt;lgl&gt;, deleted_reason &lt;chr&gt;,\n#   fast_f1generated &lt;lgl&gt;, is_accurate &lt;lgl&gt;, session_type &lt;chr&gt;"
  },
  {
    "objectID": "final_paper.html#variables-of-interest",
    "href": "final_paper.html#variables-of-interest",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "lap_time: Recorded time to complete a lap (seconds)\nlap_number: Lap number from which the telemetry data was recorded (number of laps)\ntyre_life: Number of laps completed on a set of tires (number of laps)\n\n\n\n\n\ncompound: Type of tire used (SOFT, MEDIUM, HARD)\npit_in: Whether a driver made a pit stop during a lap (binary: 0 = no pit stop, 1 = pit stop occured)"
  },
  {
    "objectID": "final_paper.html#missing-data-in-lap_time",
    "href": "final_paper.html#missing-data-in-lap_time",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "[1] \"There are  5  missing lap time values\"\n\n\nThe dataset contains five missing lap times. Out of 5 missing lap time records four records have a track status code of 41. However, no description of this code value is provided in the API. Thus, we assume that either the track was not fully cleared or conditions were not suitable for racing. The other missing record was due to a driver failing to complete a lap due to collision. Since the missing observations are less than 0.1% of the entire observation, we decided to drop these records."
  },
  {
    "objectID": "final_paper.html#exploratory-data-analysis-visualization",
    "href": "final_paper.html#exploratory-data-analysis-visualization",
    "title": "STAT 244-SC Final Paper",
    "section": "",
    "text": "Distribution of Pit Stops by Lap\nThis plot shows the frequency of pit stops across laps during the 2024 Miami Grand Prix. It helps visualize when teams tend to stop during the race. Many teams pitted to change tires during the first half of the race and the most common pit stop occurring around lap 28. This race was unique in that some drivers performed a one-stop strategy, while others went for a two-stop approach. These decisions were influenced by various factors such as track position, gaps to nearby drivers, tire condition, and more. Pit stops in the later stages of the race likely reflect either a two-stop strategy or an attempt to set the fastest lap and earn an extra point.\n\n\n\n\n\n\n\n\n\nDensity of Lap Times by Team\nThis plot shows the distribution of lap times for each team during the race. We can compare the performance and variability in lap times across different teams. For most teams, the lap times are generally under 100 seconds, with some laps approaching 110 seconds. These patterns are largely consistent across teams, though some, such as Mercedes and Williams, show a few outliers on the higher end, which indicates occasional slower laps.\n\n\n\n\n\n\n\n\n\nDistribution of Tire Life by Tire Compound\nThe plot shows the distribution of tire life (measured in laps) for each tire compound used in the race. On average, hard tires lasted very slightly longer than medium tires. Since hard and medium compounds were the most commonly used in this race, we have limited data on soft tires, roughly a quarter as much. This resulted in a narrower distribution for the soft compound. The tire compound directly affects tire life, with a trade-off between performance (speed and grip) and durability. As a result, softer compounds tend to wear out more quickly than harder ones."
  },
  {
    "objectID": "final_paper.html#research-questions",
    "href": "final_paper.html#research-questions",
    "title": "STAT 244-SC Final Paper",
    "section": "Research questions",
    "text": "Research questions\n\nWere drivers more likely to make pit stops when their lap time was longer and their tires were older compared to when their lap time was shorter and their tires were less used?\nWere drivers more likely to make pit stops when their lap times were longer, their tires were older, and considering the type of tires they were using and their progress in the race?"
  },
  {
    "objectID": "final_paper.html#linear-models",
    "href": "final_paper.html#linear-models",
    "title": "STAT 244-SC Final Paper",
    "section": "Linear Models",
    "text": "Linear Models\n\nModel 1:\n\n\\[\\mathbb{E}(pit\\_in \\mid lap\\_time,\\ tyre\\_life) = \\beta_0 + \\beta_1(lap\\_time) + \\beta_2(tyre\\_life)\\]\n\nModel 2:\n\n\\[\n\\begin{aligned}\n\\mathbb{E}(pit\\_in \\mid lap\\_time, \\ lap\\_number, \\ compound, \\ tyre\\_life)  &= \\beta_0 + \\beta_1(lap\\_time) \\\\ &+ \\beta_2(lap\\_number) + \\beta_3(compound) \\\\ &+ \\beta_4(tyre\\_life)\n\\end{aligned}\n\\]\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -0.422    0.0543       -7.77 1.74e-14\n2 lap_time     0.00429  0.000537      7.98 3.63e-15\n3 tyre_life    0.00243  0.000509      4.79 1.94e- 6\n\n\n# A tibble: 6 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -0.446    0.0544       -8.21 6.32e-16\n2 lap_time        0.00468  0.000534      8.76 7.32e-18\n3 lap_number     -0.00214  0.000387     -5.54 3.88e- 8\n4 compoundMEDIUM  0.0117   0.00936       1.25 2.10e- 1\n5 compoundSOFT    0.0312   0.0240        1.30 1.93e- 1\n6 tyre_life       0.00519  0.000698      7.44 1.97e-13\n\n\nThe regression results show that drivers were slightly more likely to make pit stops when their lap times were longer and their tires were older. In the extended model, lap time and tire age remained strong predictors and suggested that there are fewer stops later in the race with lap number having a slight negative effect. Tire compound had a small and non-significant effect. This indicates that tire compound did not meaningfully influence pit stop decisions when other factors were considered."
  },
  {
    "objectID": "final_paper.html#cross-validation",
    "href": "final_paper.html#cross-validation",
    "title": "STAT 244-SC Final Paper",
    "section": "Cross Validation",
    "text": "Cross Validation\nCross-validation is a statistical method used to evaluate how well a model performs by splitting the data into multiple subsets to train the model on some subsets and validate it on the remaining subsets.\nGoal: Provide a more reliable and unbiased estimate of a model’s performance predicting new data, in order to detect overfitting and improve model generalization\n\nDividing data into test set and training set\nk-fold CV: We can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\nFit a model using the data in the other \\(k-1\\) folds (training).\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\nCalculate the MAE/MSE for fold \\(j\\) (testing):\n\nCombine this information into one measure of model quality\n\n\n\nError metric to use\nMean absolute error (MAE) of an estimator measures the absolute difference between the predicted values and the actual values in the dataset. Its advantage is that its\n\n\\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\)\n\\(\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\)\n\nMean squared error (MSE) of an estimator measures the average squared difference between the predicted values and the actual values in the dataset.\n\n\\(\\text{MSE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} (y_i - \\hat{y}_i)^2\\)\n\\(\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MSE}_j\\)\n\n\nMAE vs. MSE\nThe advantage of using MAE is that it’s more robust to outliers, giving equal weight to all errors. Thus, it’s more suitable when outliers are not a significant concern.\nOn the other hand, MSE gives more weight to larger errors than smaller ones, making it highly sensitive to outliers. MSE is more suitable when the risk of prediction mistakes is crucial and the goal is to minimize the risk of errors.\nSince outliers are less of a concern for us as they don’t lead to any life threatening or other major issues, we prioritize models that are directly interpretable. Our data is less common and less familiar to many people, so we decided to choose a model based on MAE.\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard      0.0505\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard      0.0587"
  },
  {
    "objectID": "final_paper.html#k-fold-cv-implementation-for-different-values-of-k",
    "href": "final_paper.html#k-fold-cv-implementation-for-different-values-of-k",
    "title": "STAT 244-SC Final Paper",
    "section": "k-fold CV implementation for different values of k",
    "text": "k-fold CV implementation for different values of k\n\nCase 1: k=5\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0507     5 0.00159 Preprocessor1_Model1\n2 rmse    standard   0.152      5 0.00536 Preprocessor1_Model1\n\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [884/222]&gt; Fold1 mae     standard      0.0477 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [885/221]&gt; Fold2 mae     standard      0.0560 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [885/221]&gt; Fold3 mae     standard      0.0528 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [885/221]&gt; Fold4 mae     standard      0.0486 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [885/221]&gt; Fold5 mae     standard      0.0485 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.048) and worst for fold 2 (0.056).\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0592     5 0.00161 Preprocessor1_Model1\n2 rmse    standard   0.150      5 0.00490 Preprocessor1_Model1\n\n\n# A tibble: 5 × 7\n  splits            id    .metric .estimator .estimate .config          .notes  \n  &lt;list&gt;            &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;list&gt;  \n1 &lt;split [884/222]&gt; Fold1 mae     standard      0.0533 Preprocessor1_M… &lt;tibble&gt;\n2 &lt;split [885/221]&gt; Fold2 mae     standard      0.0621 Preprocessor1_M… &lt;tibble&gt;\n3 &lt;split [885/221]&gt; Fold3 mae     standard      0.0617 Preprocessor1_M… &lt;tibble&gt;\n4 &lt;split [885/221]&gt; Fold4 mae     standard      0.0585 Preprocessor1_M… &lt;tibble&gt;\n5 &lt;split [885/221]&gt; Fold5 mae     standard      0.0606 Preprocessor1_M… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.053) and worst for fold 2 (0.062).\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0507 0.00356\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0592 0.00360\n\n\nIn-sample and 5-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n5-fold CV MAE\nIn-sample SD\n5-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05073\n0.15247\n0.00356\n\n\nmodel_2\n0.05975\n0.05922\n0.15035\n0.00360\n\n\n\n5-fold cross-validation was used to assess the performance of two models predicting pit stops. Model 1, using only lap time and tire life, achieved a mean MAE of 0.05073 with a low standard deviation (0.00356). Model 2, which adds lap number and tire compound, had a higher mean MAE of 0.05922 with a similar standard deviation (0.00360).\nAlthough Model 2 includes more predictors, it performed slightly worse than Model 1 in both cross-validation and in-sample metrics. This suggests that the additional variables do not improve prediction. Model 1 is therefore more accurate and efficient for predicting pit stops.\n\n\nCase 2: k=10\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0510    10 0.00294 Preprocessor1_Model1\n2 rmse    standard   0.150     10 0.0109  Preprocessor1_Model1\n\n\n# A tibble: 10 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [995/111]&gt; Fold01 mae     standard      0.0368 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [995/111]&gt; Fold02 mae     standard      0.0544 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [995/111]&gt; Fold03 mae     standard      0.0614 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [995/111]&gt; Fold04 mae     standard      0.0472 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [995/111]&gt; Fold05 mae     standard      0.0379 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [995/111]&gt; Fold06 mae     standard      0.0602 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [996/110]&gt; Fold07 mae     standard      0.0600 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [996/110]&gt; Fold08 mae     standard      0.0434 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [996/110]&gt; Fold09 mae     standard      0.0505 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [996/110]&gt; Fold10 mae     standard      0.0581 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, the MAE was best for fold 1 with an MAE of approximately 0.037 and worst for fold 3 with an MAE of 0.061 approximately.\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0594    10 0.00262 Preprocessor1_Model1\n2 rmse    standard   0.148     10 0.0104  Preprocessor1_Model1\n\n\n# A tibble: 10 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [995/111]&gt; Fold01 mae     standard      0.0436 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [995/111]&gt; Fold02 mae     standard      0.0616 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [995/111]&gt; Fold03 mae     standard      0.0698 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [995/111]&gt; Fold04 mae     standard      0.0566 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [995/111]&gt; Fold05 mae     standard      0.0518 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [995/111]&gt; Fold06 mae     standard      0.0658 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [996/110]&gt; Fold07 mae     standard      0.0655 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [996/110]&gt; Fold08 mae     standard      0.0521 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [996/110]&gt; Fold09 mae     standard      0.0601 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [996/110]&gt; Fold10 mae     standard      0.0671 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 1 (0.044) and worst for fold 3 (0.070).\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0510 0.00931\n\n\n# A tibble: 1 × 2\n    mean      sd\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.0594 0.00829\n\n\nIn-sample and 10-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n10-fold CV MAE\nIn-sample SD\n10-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05100\n0.15247\n0.00931\n\n\nmodel_2\n0.05975\n0.05939\n0.15035\n0.00829\n\n\n\nWith 10-fold cross-validation, Model 1 had a mean MAE of 0.0510, while Model 2 had a slightly higher MAE of 0.0594. Both models showed low standard deviations approximately 0.009. As in the 5-fold case, Model 1 remained slightly more accurate and stable than Model 2.\n\n\nCase 3: k = 20\nModel 1\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0509    20 0.00399 Preprocessor1_Model1\n2 rmse    standard   0.140     20 0.0142  Preprocessor1_Model1\n\n\n# A tibble: 20 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [1050/56]&gt; Fold01 mae     standard      0.0451 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [1050/56]&gt; Fold02 mae     standard      0.0519 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [1050/56]&gt; Fold03 mae     standard      0.0509 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [1050/56]&gt; Fold04 mae     standard      0.0658 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [1050/56]&gt; Fold05 mae     standard      0.0439 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [1050/56]&gt; Fold06 mae     standard      0.0412 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [1051/55]&gt; Fold07 mae     standard      0.0602 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [1051/55]&gt; Fold08 mae     standard      0.0429 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [1051/55]&gt; Fold09 mae     standard      0.0402 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [1051/55]&gt; Fold10 mae     standard      0.0263 Preprocessor1… &lt;tibble&gt;\n11 &lt;split [1051/55]&gt; Fold11 mae     standard      0.0266 Preprocessor1… &lt;tibble&gt;\n12 &lt;split [1051/55]&gt; Fold12 mae     standard      0.0585 Preprocessor1… &lt;tibble&gt;\n13 &lt;split [1051/55]&gt; Fold13 mae     standard      0.0704 Preprocessor1… &lt;tibble&gt;\n14 &lt;split [1051/55]&gt; Fold14 mae     standard      0.0274 Preprocessor1… &lt;tibble&gt;\n15 &lt;split [1051/55]&gt; Fold15 mae     standard      0.0302 Preprocessor1… &lt;tibble&gt;\n16 &lt;split [1051/55]&gt; Fold16 mae     standard      0.0825 Preprocessor1… &lt;tibble&gt;\n17 &lt;split [1051/55]&gt; Fold17 mae     standard      0.0591 Preprocessor1… &lt;tibble&gt;\n18 &lt;split [1051/55]&gt; Fold18 mae     standard      0.0429 Preprocessor1… &lt;tibble&gt;\n19 &lt;split [1051/55]&gt; Fold19 mae     standard      0.0611 Preprocessor1… &lt;tibble&gt;\n20 &lt;split [1051/55]&gt; Fold20 mae     standard      0.0901 Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 10 (0.026) and worst for fold 20 (0.090).\nModel 2\n\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.0593    20 0.00398 Preprocessor1_Model1\n2 rmse    standard   0.139     20 0.0134  Preprocessor1_Model1\n\n\n# A tibble: 20 × 7\n   splits            id     .metric .estimator .estimate .config        .notes  \n   &lt;list&gt;            &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;list&gt;  \n 1 &lt;split [1050/56]&gt; Fold01 mae     standard      0.0508 Preprocessor1… &lt;tibble&gt;\n 2 &lt;split [1050/56]&gt; Fold02 mae     standard      0.0623 Preprocessor1… &lt;tibble&gt;\n 3 &lt;split [1050/56]&gt; Fold03 mae     standard      0.0564 Preprocessor1… &lt;tibble&gt;\n 4 &lt;split [1050/56]&gt; Fold04 mae     standard      0.0755 Preprocessor1… &lt;tibble&gt;\n 5 &lt;split [1050/56]&gt; Fold05 mae     standard      0.0596 Preprocessor1… &lt;tibble&gt;\n 6 &lt;split [1050/56]&gt; Fold06 mae     standard      0.0535 Preprocessor1… &lt;tibble&gt;\n 7 &lt;split [1051/55]&gt; Fold07 mae     standard      0.0652 Preprocessor1… &lt;tibble&gt;\n 8 &lt;split [1051/55]&gt; Fold08 mae     standard      0.0492 Preprocessor1… &lt;tibble&gt;\n 9 &lt;split [1051/55]&gt; Fold09 mae     standard      0.0474 Preprocessor1… &lt;tibble&gt;\n10 &lt;split [1051/55]&gt; Fold10 mae     standard      0.0324 Preprocessor1… &lt;tibble&gt;\n11 &lt;split [1051/55]&gt; Fold11 mae     standard      0.0347 Preprocessor1… &lt;tibble&gt;\n12 &lt;split [1051/55]&gt; Fold12 mae     standard      0.0630 Preprocessor1… &lt;tibble&gt;\n13 &lt;split [1051/55]&gt; Fold13 mae     standard      0.0818 Preprocessor1… &lt;tibble&gt;\n14 &lt;split [1051/55]&gt; Fold14 mae     standard      0.0362 Preprocessor1… &lt;tibble&gt;\n15 &lt;split [1051/55]&gt; Fold15 mae     standard      0.0405 Preprocessor1… &lt;tibble&gt;\n16 &lt;split [1051/55]&gt; Fold16 mae     standard      0.0838 Preprocessor1… &lt;tibble&gt;\n17 &lt;split [1051/55]&gt; Fold17 mae     standard      0.0652 Preprocessor1… &lt;tibble&gt;\n18 &lt;split [1051/55]&gt; Fold18 mae     standard      0.0537 Preprocessor1… &lt;tibble&gt;\n19 &lt;split [1051/55]&gt; Fold19 mae     standard      0.0724 Preprocessor1… &lt;tibble&gt;\n20 &lt;split [1051/55]&gt; Fold20 mae     standard      0.102  Preprocessor1… &lt;tibble&gt;\n\n\nBased on the random folds above, MAE was best for fold 10 (0.032) and worst for fold 20 (0.101).\n\n\n# A tibble: 1 × 2\n    mean     sd\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0509 0.0178\n\n\n# A tibble: 1 × 2\n    mean     sd\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 0.0593 0.0178\n\n\nIn-sample and 20-fold CV MAE and standard deviation for both models.\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n20-fold CV MAE\nIn-sample SD\n20-fold CV SD\n\n\n\n\nmodel_1\n0.05045\n0.05086\n0.15247\n0.01785\n\n\nmodel_2\n0.05975\n0.05925\n0.15035\n0.01781\n\n\n\nIn the 20-fold CV setup, Model 1 performed better than Model 2 with a lower mean MAE, 0.05086 and 0.05925, respectively. Even with increased fold, the simpler model generalized better across the dataset.\n\n\nComparison between different values of k\n\n\n\n\n\n\n\n\n\n\nModel\nIn-sample MAE\n5-fold CV MAE\n10-fold CV MAE\n20-fold CV MAE\n\n\n\n\nmodel_1\n0.05045\n0.05073\n0.05100\n0.05086\n\n\nmodel_2\n0.05975\n0.05922\n0.05939\n0.05925\n\n\n\nAcross all cross-validation settings (5, 10, and 20-fold), Model 1 consistently showed lower MAE than Model 2. The differences were small but consistent and this suggests that Model 1 is a better model than Model 2 in predicting pit-stops.\nTherefore, our final model based on the smallest CV error is:\n\\[\\mathbb{E}(pit\\_in \\mid lap\\_time,\\ tyre\\_life) = \\beta_0 + \\beta_1(lap\\_time) + \\beta_2(tyre\\_life)\\]"
  },
  {
    "objectID": "final_paper.html#variables-of-interest-1",
    "href": "final_paper.html#variables-of-interest-1",
    "title": "STAT 244-SC Final Paper",
    "section": "Variables of Interest",
    "text": "Variables of Interest\n\nPredictors:\n\nlap_time: Recorded time to complete a lap (seconds).\nlap_number: Lap number from which the telemetry data was recorded (number of laps).\ntyre_life: Number of laps completed on a set of tires (number of laps).\ncompound: Type of tire used (SOFT, MEDIUM, HARD).\n\n\n\nResponse Variable:\n\npit_in: Whether a driver made a pit stop during a lap where 1 indicates pit stop occurred, and 0 otherwise\n\\[\\begin{align*}\nY_i &= \\begin{cases} 1 & \\text{ if a driver pitted on a lap } \\\\ 0 & \\text{ otherwise (i.e., the driver did not pit on lap)} \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "final_paper.html#our-logistic-regression-model",
    "href": "final_paper.html#our-logistic-regression-model",
    "title": "STAT 244-SC Final Paper",
    "section": "Our Logistic Regression Model",
    "text": "Our Logistic Regression Model\nWe are interested in determining the probability of making a pit stop during the 2024 Miami Grand Prix, considering factors such as lap time, track progress, tire age, and the type of tire used.\n\\[\n\\begin{aligned}\n\\log(odds(pit\\_in \\mid lap\\_time, \\ lap\\_number, \\ tyre\\_life, \\ compound)) &= \\beta_0 + \\beta_1 (lap\\_time) \\\\ &+ \\beta_2(lap\\_number) + \\beta_3 (tyre\\_life) \\\\ &+ \\beta_4 \\ I(compound = MEDIUM) \\\\ &+ \\beta_5 \\ I(compound = SOFT)\n\\end{aligned}\n\\]\n\n\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -18.48162    2.27137  -8.137 4.06e-16 ***\nlap_time         0.13739    0.02007   6.846 7.58e-12 ***\nlap_number      -0.16001    0.03630  -4.408 1.04e-05 ***\ntyre_life        0.27508    0.04580   6.006 1.91e-09 ***\ncompoundMEDIUM   0.49495    0.49718   0.996    0.319    \ncompoundSOFT     1.86135    1.17923   1.578    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 261.16  on 1105  degrees of freedom\nResidual deviance: 176.46  on 1100  degrees of freedom\nAIC: 188.46\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nInterpretation of exponentiated \\(\\hat{\\beta}\\) coefficients\n\n\n   (Intercept)       lap_time     lap_number      tyre_life compoundMEDIUM \n  9.408757e-09   1.147280e+00   8.521343e-01   1.316630e+00   1.640419e+00 \n  compoundSOFT \n  6.432386e+00 \n\n\n\n\\(\\exp(\\beta_0)\\): The odds of a driver making a pit stop during a lap, when lap time is 0 seconds, lap number is 0, 0 laps have been completed on the current set of tires, and the HARD compound is, is approximately \\(9.4088 \\times 10^{-9}\\).\n\\(\\exp(\\beta_1)\\): For every of 1 second increase in lap time, the odds of a driver pitting increase by a factor of 1.1473.\n\\(\\exp(\\beta_2)\\): For every additional lap (i.e., increase of 1 in the lap number), we expect the odds of a driver pitting to increase by a factor of 0.8521.\n\\(\\exp(\\beta_3)\\): For each additional lap completed on the current set of tires, the odds of a driver pitting increase by a factor of 1.3166.\n\\(\\exp(\\beta_4)\\): When using MEDIUM compound tires instead of HARD, the odds of a driver pitting increase by a factor of 1.6404, holding all other variables constant.\n\\(\\exp(\\beta_5)\\): When using SOFT compound tires instead of HARD, we expect the odds of a driver pitting to increase by a factor of 6.4324, holding all other variables constant.\n\n\nMathematically derive \\(\\exp(\\beta_1)\\)\n\\[\n\\begin{aligned}\n&\\log(odds(pit\\_in \\mid lap\\_time = a)) = -18.4816 + 0.1374a\n\\\\\n\\\\\n&\\log(odds(pit\\_in \\mid lap\\_time = a+1)) = -18.4816 + 0.1374(a+1)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n& \\log\\left( \\frac{odds(pit\\_in \\mid lap\\_time = a+1)}{odds(pit\\_in \\mid lap\\_time = a)} \\right)\\\\\n&= \\log(odds(pit\\_in \\mid lap\\_time = a+1)) - \\log(odds(pit\\_in \\mid lap\\_time = a)) \\\\\n&= (-18.4816 + 0.1374(a+1)) - (-18.4816 + 0.1374) \\\\\n&= 0.1374  \\\\\n&= \\hat{\\beta_1}\n\\end{aligned}\n\\]\nTherefore, \\(\\exp(\\beta_1) = e^{0.1374} = 1.1473\\)\n\n\n\nPredicting High Probability of a Pit Stop\nTo predict a probability of a driver making a pit stop that is very close to 1, we need to input extreme values for the predictors. Based on the five-number summary of our data, we use the following scenario: a lap time of 148.74 seconds, lap number 57, SOFT compound, and a tire age of 45 laps.\n\n\n    lap_time        lap_number      compound     tyre_life    \n Min.   : 90.63   Min.   : 1.00   HARD  :500   Min.   : 1.00  \n 1st Qu.: 92.38   1st Qu.:14.00   MEDIUM:562   1st Qu.: 7.00  \n Median : 93.28   Median :28.00   SOFT  : 44   Median :13.50  \n Mean   : 96.00   Mean   :28.62                Mean   :14.78  \n 3rd Qu.: 94.29   3rd Qu.:43.00                3rd Qu.:22.00  \n Max.   :148.74   Max.   :57.00                Max.   :45.00  \n     pit_in        pit_in_fac\n Min.   :0.00000   0:1078    \n 1st Qu.:0.00000   1:  28    \n Median :0.00000             \n Mean   :0.02532             \n 3rd Qu.:0.00000             \n Max.   :1.00000             \n\n\n\n\n        1 \n0.7308921 \n\n\nUsing our logistic regression model, we estimate the probability of a pit stop under these conditions to be approximately 0.731. This indicates a high likelihood of a pit stop given these extreme race conditions.\n\n\nPredicting Pit Stops with our Logistic Regression Model\n\nEstimate the probability of a driver making a pit stop on a lap with the following conditions: 96.00 seconds lap time, 28th lap, 14.78 laps completed on a set of HARD tires.\n\n\n        1 \n0.5008283 \n\n\n\nThere is approximately a 50.08% probability that a driver will make a pit stop on this lap when using HARD tires, holding all other variables constant.\n\n\nEstimate the probability of a driver making a pit stop on a lap under the same conditions as above but using a set of MEDIUM tires.\n\n\n        1 \n0.5013559 \n\n\n\nWith MEDIUM tires, the probability of making a pit stop increases to 50.14%.\n\n\nEstimate the probability of a driver making a pit stop on a lap under the same conditions as above but using a set of SOFT tires.\n\n\n        1 \n0.5052337 \n\n\n\nWith SOFT tires, the probability increases slightly to 50.52%.\nWhile all the other variables stay the same, we predict that the probability a driver to made a pit stop is higher if the driver is on a set of SOFT tires compared to other compounds."
  },
  {
    "objectID": "final_paper.html#proscons-of-logistic-regression-vs.-regular-linear-regression",
    "href": "final_paper.html#proscons-of-logistic-regression-vs.-regular-linear-regression",
    "title": "STAT 244-SC Final Paper",
    "section": "Pros/Cons of logistic regression vs. regular linear regression",
    "text": "Pros/Cons of logistic regression vs. regular linear regression\n\nLogistic Regression\n\n\n\n\n\n\n\nPros\nSince logistic regression is based on a Bernoulli/binomial likelihood, it is a natural model for binary outcomes.\nCoefficients are interpretable in terms of odds ratios (with log-odds as the linear predictor).\n\n\nCons\nThe relationship between predictors and the probability is not linear.\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nPros\nStraightforward linear regression\nEasy to interpret the coefficients\n\n\nCons\nCannot gaurantee that the predicted probabilities to be between 0 and 1."
  }
]